{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"hf_xxxxxxxxxx\")\n",
    "\n",
    "natural_questions_dataset = load_dataset(\"sentence-transformers/natural-questions\", split=\"train\")\n",
    "\n",
    "def fix_tokens(token_matrix):\n",
    "    for i in range(token_matrix.shape[0]):\n",
    "        for j in range(token_matrix.shape[1]):\n",
    "            token_matrix[i, j] = token_matrix[i, j].replace(\"$\", \"d\").strip()\n",
    "\n",
    "def get_custom_prompt(index=0):\n",
    "    item = natural_questions_dataset[index]\n",
    "    question = item[\"query\"] \n",
    "    answer = item[\"answer\"]  \n",
    "    prompt = f\"Question: {question}\\nAnswer:\"\n",
    "    return prompt, answer, question\n",
    "\n",
    "\n",
    "def calculate_average_stability(consecutive_matches):\n",
    "    total = sum(consecutive_matches.values())\n",
    "    num_tokens = len(consecutive_matches)\n",
    "    return total / num_tokens if num_tokens > 0 else 0\n",
    "\n",
    "def count_consecutive_matches_before_layer_33(tokens_matrix):\n",
    "    num_layers, num_tokens = tokens_matrix.shape\n",
    "    final_tokens = [token.strip().lower() for token in tokens_matrix[0, :]]\n",
    "    consecutive_matches = {}\n",
    "\n",
    "    for token_idx in range(num_tokens):\n",
    "        target_token = final_tokens[token_idx]\n",
    "        count = 0\n",
    "        for layer_idx in range(0, num_layers):\n",
    "            predicted_token = tokens_matrix[layer_idx, token_idx].strip().lower()\n",
    "            if predicted_token == target_token:\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "        consecutive_matches[f\"Position {token_idx} ('{target_token}')\"] = count\n",
    "    return consecutive_matches\n",
    "\n",
    "def calculate_next_token_accuracy(model, tokenizer, inputs):\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits[:, :-1], dim=-1)\n",
    "    target_ids = input_ids[:, 1:]\n",
    "\n",
    "    correct = (predicted_ids == target_ids).sum().item()\n",
    "    total = target_ids.numel()\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "def logit_lens_heatmap_from_file(model_id=\"FreedomIntelligence/HuatuoGPT-o1-8B\", index=0):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=torch.float16, device_map=\"auto\"\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    prompt, expected_answer, question_text = get_custom_prompt(index=index)\n",
    "    print(f\"\\n Spørgsmål: {question_text}\")\n",
    "    print(f\" Forventet svar: {expected_answer}\")\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, output_scores=True)\n",
    "\n",
    "    all_hidden = outputs.hidden_states\n",
    "    num_layers = model.config.num_hidden_layers + 1\n",
    "    lm_head_weight = model.get_output_embeddings().weight.to(model.device)\n",
    "\n",
    "    tokens_matrix = []\n",
    "    probs_matrix = []\n",
    "\n",
    "    for layer in all_hidden:\n",
    "        logits = torch.matmul(layer, lm_head_weight.T)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        top_ids = torch.argmax(probs, dim=-1)\n",
    "        top_tokens = [tokenizer.decode(top_ids[0, i]) for i in range(top_ids.size(1))]\n",
    "        H = entropy(probs.detach().cpu().numpy(), axis=-1)\n",
    "        tokens_matrix.append(top_tokens)\n",
    "        probs_matrix.append(list(H[0]))\n",
    "\n",
    "    tokens_matrix = np.array(tokens_matrix)[::-1, :]\n",
    "    probs_matrix = np.array(probs_matrix)[::-1, :]\n",
    "\n",
    "    fix_tokens(tokens_matrix)\n",
    "\n",
    "    expected_tokens = [tokenizer.decode([t]) for t in inputs['input_ids'][0]]\n",
    "    expected_tokens = [token.strip() for token in expected_tokens]\n",
    "\n",
    "    plt.figure(figsize=(1 + len(expected_tokens), 10))\n",
    "    sns.heatmap(\n",
    "        probs_matrix,\n",
    "        annot=tokens_matrix,\n",
    "        fmt=\"\",\n",
    "        cmap=\"YlGnBu\",\n",
    "        xticklabels=expected_tokens,\n",
    "        yticklabels=[f\"L{i}\" for i in range(num_layers, 0, -1)],\n",
    "        cbar_kws={'label': 'Entropy'}\n",
    "    )\n",
    "    plt.title(f\"Logit Lens Heatmap\\nModel: {model_id}\\nQ: {question_text}\")\n",
    "    plt.xlabel(\"Generated Token\")\n",
    "    plt.ylabel(\"Transformer Layer\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return tokens_matrix, expected_tokens, question_text, model, tokenizer, inputs\n",
    "\n",
    "index = 0  \n",
    "\n",
    "tokens_matrix, expected_tokens, question_text, model, tokenizer, inputs = logit_lens_heatmap_from_file(\n",
    "    model_id=\"FreedomIntelligence/HuatuoGPT-o1-8B\",\n",
    "    index=index\n",
    ")\n",
    "\n",
    "consecutive_matches = count_consecutive_matches_before_layer_33(tokens_matrix)\n",
    "avg_stability = calculate_average_stability(consecutive_matches)\n",
    "accuracy = calculate_next_token_accuracy(model, tokenizer, inputs)\n",
    "\n",
    "print(\"\\n Antal sammenhængende matches op til lag 33:\")\n",
    "for token, count in consecutive_matches.items():\n",
    "    print(f\"{token.ljust(40)} | Matches: {count}\")\n",
    "\n",
    "print(f\"\\n Gennemsnitlig stabilitet: {avg_stability:.4f}\")\n",
    "print(f\" Next-token accuracy      : {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"hf_xxxxxxxxxx\")\n",
    "\n",
    "natural_questions_dataset = load_dataset(\"sentence-transformers/natural-questions\", split=\"train\")\n",
    "\n",
    "def fix_tokens(token_matrix):\n",
    "    for i in range(token_matrix.shape[0]):\n",
    "        for j in range(token_matrix.shape[1]):\n",
    "            token_matrix[i, j] = token_matrix[i, j].replace(\"$\", \"d\").strip()\n",
    "\n",
    "def get_custom_prompt(index=0):\n",
    "    item = natural_questions_dataset[index]\n",
    "    question = item[\"query\"]  \n",
    "    answer = item[\"answer\"]  \n",
    "    prompt = f\"Question: {question}\\nAnswer:\"\n",
    "    return prompt, answer, question\n",
    "\n",
    "\n",
    "def calculate_average_stability(consecutive_matches):\n",
    "    total = sum(consecutive_matches.values())\n",
    "    num_tokens = len(consecutive_matches)\n",
    "    return total / num_tokens if num_tokens > 0 else 0\n",
    "\n",
    "def count_consecutive_matches_before_layer_33(tokens_matrix):\n",
    "    num_layers, num_tokens = tokens_matrix.shape\n",
    "    final_tokens = [token.strip().lower() for token in tokens_matrix[0, :]]\n",
    "    consecutive_matches = {}\n",
    "\n",
    "    for token_idx in range(num_tokens):\n",
    "        target_token = final_tokens[token_idx]\n",
    "        count = 0\n",
    "        for layer_idx in range(0, num_layers):\n",
    "            predicted_token = tokens_matrix[layer_idx, token_idx].strip().lower()\n",
    "            if predicted_token == target_token:\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "        consecutive_matches[f\"Position {token_idx} ('{target_token}')\"] = count\n",
    "    return consecutive_matches\n",
    "\n",
    "def calculate_next_token_accuracy(model, tokenizer, inputs):\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits[:, :-1], dim=-1)\n",
    "    target_ids = input_ids[:, 1:]\n",
    "\n",
    "    correct = (predicted_ids == target_ids).sum().item()\n",
    "    total = target_ids.numel()\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "def logit_lens_heatmap_from_file(model_id=\"meta-llama/Llama-3.1-8B\", index=0):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=torch.float16, device_map=\"auto\"\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    prompt, expected_answer, question_text = get_custom_prompt(index=index)\n",
    "    print(f\"\\n Spørgsmål: {question_text}\")\n",
    "    print(f\" Forventet svar: {expected_answer}\")\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, output_scores=True)\n",
    "\n",
    "    all_hidden = outputs.hidden_states\n",
    "    num_layers = model.config.num_hidden_layers + 1\n",
    "    lm_head_weight = model.get_output_embeddings().weight.to(model.device)\n",
    "\n",
    "    tokens_matrix = []\n",
    "    probs_matrix = []\n",
    "\n",
    "    for layer in all_hidden:\n",
    "        logits = torch.matmul(layer, lm_head_weight.T)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        top_ids = torch.argmax(probs, dim=-1)\n",
    "        top_tokens = [tokenizer.decode(top_ids[0, i]) for i in range(top_ids.size(1))]\n",
    "        H = entropy(probs.detach().cpu().numpy(), axis=-1)\n",
    "        tokens_matrix.append(top_tokens)\n",
    "        probs_matrix.append(list(H[0]))\n",
    "\n",
    "    tokens_matrix = np.array(tokens_matrix)[::-1, :]\n",
    "    probs_matrix = np.array(probs_matrix)[::-1, :]\n",
    "\n",
    "    fix_tokens(tokens_matrix)\n",
    "\n",
    "    expected_tokens = [tokenizer.decode([t]) for t in inputs['input_ids'][0]]\n",
    "    expected_tokens = [token.strip() for token in expected_tokens]\n",
    "\n",
    "    plt.figure(figsize=(1 + len(expected_tokens), 10))\n",
    "    sns.heatmap(\n",
    "        probs_matrix,\n",
    "        annot=tokens_matrix,\n",
    "        fmt=\"\",\n",
    "        cmap=\"YlGnBu\",\n",
    "        xticklabels=expected_tokens,\n",
    "        yticklabels=[f\"L{i}\" for i in range(num_layers, 0, -1)],\n",
    "        cbar_kws={'label': 'Entropy'}\n",
    "    )\n",
    "    plt.title(f\"Logit Lens Heatmap\\nModel: {model_id}\\nQ: {question_text}\")\n",
    "    plt.xlabel(\"Generated Token\")\n",
    "    plt.ylabel(\"Transformer Layer\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return tokens_matrix, expected_tokens, question_text, model, tokenizer, inputs\n",
    "\n",
    "index = 0  \n",
    "\n",
    "tokens_matrix, expected_tokens, question_text, model, tokenizer, inputs = logit_lens_heatmap_from_file(\n",
    "    model_id=\"meta-llama/Llama-3.1-8B\",\n",
    "    index=index\n",
    ")\n",
    "\n",
    "consecutive_matches = count_consecutive_matches_before_layer_33(tokens_matrix)\n",
    "avg_stability = calculate_average_stability(consecutive_matches)\n",
    "accuracy = calculate_next_token_accuracy(model, tokenizer, inputs)\n",
    "\n",
    "print(\"\\n Antal sammenhængende matches op til lag 33:\")\n",
    "for token, count in consecutive_matches.items():\n",
    "    print(f\"{token.ljust(40)} | Matches: {count}\")\n",
    "\n",
    "print(f\"\\n Gennemsnitlig stabilitet: {avg_stability:.4f}\")\n",
    "print(f\" Next-token accuracy      : {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_questions(model_id, indices):\n",
    "    print(f\"\\n Evaluering for model: {model_id}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=torch.float16, device_map=\"auto\"\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    all_accuracies = []\n",
    "    all_stabilities = []\n",
    "\n",
    "    for idx in indices:\n",
    "        try:\n",
    "            prompt, expected_answer, question_text = get_custom_prompt(index=idx)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "            hidden_states = outputs.hidden_states\n",
    "            lm_head_weight = model.get_output_embeddings().weight.to(model.device)\n",
    "\n",
    "            tokens_matrix = []\n",
    "            for layer in hidden_states:\n",
    "                logits = torch.matmul(layer, lm_head_weight.T)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                top_ids = torch.argmax(probs, dim=-1)\n",
    "                top_tokens = [tokenizer.decode(top_ids[0, i]) for i in range(top_ids.size(1))]\n",
    "                tokens_matrix.append(top_tokens)\n",
    "\n",
    "            tokens_matrix = np.array(tokens_matrix)[::-1, :]\n",
    "            fix_tokens(tokens_matrix)\n",
    "\n",
    "            matches = count_consecutive_matches_before_layer_33(tokens_matrix)\n",
    "            avg_stab = calculate_average_stability(matches)\n",
    "            all_stabilities.append(avg_stab)\n",
    "\n",
    "            acc = calculate_next_token_accuracy(model, tokenizer, inputs)\n",
    "            all_accuracies.append(acc)\n",
    "\n",
    "            print(f\"[{idx}] Stability: {avg_stab:.4f} | Accuracy: {acc:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Fejl ved spørgsmål {idx}: {e}\")\n",
    "\n",
    "    \n",
    "    avg_stability = sum(all_stabilities) / len(all_stabilities) if all_stabilities else 0.0\n",
    "    avg_accuracy = sum(all_accuracies) / len(all_accuracies) if all_accuracies else 0.0\n",
    "\n",
    "    print(f\"\\n Resultater for {model_id}\")\n",
    "    print(f\"Stabilitet (avg): {avg_stability:.4f}\")\n",
    "    print(f\"Accuracy (avg):   {avg_accuracy:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"FreedomIntelligence/HuatuoGPT-o1-8B\",\n",
    "    \"meta-llama/Llama-3.1-8B\"\n",
    "]\n",
    "\n",
    "indices = list(range(0, 5000)) \n",
    "\n",
    "for model_id in models:\n",
    "    evaluate_model_on_questions(model_id, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"hf_xxxxxxxxxx\")\n",
    "\n",
    "natural_questions_dataset = load_dataset(\"sentence-transformers/natural-questions\", split=\"train\")\n",
    "\n",
    "def fix_tokens(token_matrix):\n",
    "    for i in range(token_matrix.shape[0]):\n",
    "        for j in range(token_matrix.shape[1]):\n",
    "            token_matrix[i, j] = token_matrix[i, j].replace(\"$\", \"d\").strip()\n",
    "\n",
    "def get_custom_prompt(index=0):\n",
    "    item = natural_questions_dataset[index]\n",
    "    question = item[\"query\"]\n",
    "    answer = item[\"answer\"]\n",
    "    prompt = f\"Question: {question}\\nAnswer: {answer}\"\n",
    "    return prompt, answer, question\n",
    "\n",
    "def calculate_average_stability(consecutive_matches):\n",
    "    total = sum(consecutive_matches.values())\n",
    "    num_tokens = len(consecutive_matches)\n",
    "    return total / num_tokens if num_tokens > 0 else 0\n",
    "\n",
    "def count_consecutive_matches_before_layer_33(tokens_matrix):\n",
    "    num_layers, num_tokens = tokens_matrix.shape\n",
    "    final_tokens = [token.strip().lower() for token in tokens_matrix[0, :]]\n",
    "    consecutive_matches = {}\n",
    "\n",
    "    for token_idx in range(num_tokens):\n",
    "        target_token = final_tokens[token_idx]\n",
    "        count = 0\n",
    "        for layer_idx in range(0, num_layers):\n",
    "            predicted_token = tokens_matrix[layer_idx, token_idx].strip().lower()\n",
    "            if predicted_token == target_token:\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "        consecutive_matches[f\"Position {token_idx} ('{target_token}')\"] = count\n",
    "    return consecutive_matches\n",
    "\n",
    "def calculate_next_token_accuracy(model, tokenizer, inputs):\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits[:, :-1], dim=-1)\n",
    "    target_ids = input_ids[:, 1:]\n",
    "\n",
    "    correct = (predicted_ids == target_ids).sum().item()\n",
    "    total = target_ids.numel()\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "def logit_lens_heatmap_from_file(model_id=\"meta-llama/Llama-3.1-8B\", index=0):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=torch.float16, device_map=\"auto\"\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    prompt, expected_answer, question_text = get_custom_prompt(index=index)\n",
    "    print(f\"\\n Spørgsmål: {question_text}\")\n",
    "    print(f\" Forventet svar: {expected_answer}\")\n",
    "\n",
    "    full_input = prompt\n",
    "    inputs = tokenizer(full_input, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, output_scores=True)\n",
    "\n",
    "    all_hidden = outputs.hidden_states\n",
    "    num_layers = model.config.num_hidden_layers + 1\n",
    "    lm_head_weight = model.get_output_embeddings().weight.to(model.device)\n",
    "\n",
    "    tokens_matrix = []\n",
    "    probs_matrix = []\n",
    "\n",
    "    for layer in all_hidden:\n",
    "        logits = torch.matmul(layer, lm_head_weight.T)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        top_ids = torch.argmax(probs, dim=-1)\n",
    "        top_tokens = [tokenizer.decode(top_ids[0, i]) for i in range(top_ids.size(1))]\n",
    "        H = entropy(probs.detach().cpu().numpy(), axis=-1)\n",
    "        tokens_matrix.append(top_tokens)\n",
    "        probs_matrix.append(list(H[0]))\n",
    "\n",
    "    tokens_matrix = np.array(tokens_matrix)[::-1, :]\n",
    "    probs_matrix = np.array(probs_matrix)[::-1, :]\n",
    "\n",
    "    fix_tokens(tokens_matrix)\n",
    "\n",
    "    expected_tokens = [tokenizer.decode([t]) for t in inputs['input_ids'][0]]\n",
    "    expected_tokens = [token.strip() for token in expected_tokens]\n",
    "\n",
    "    plt.figure(figsize=(1 + len(expected_tokens), 10))\n",
    "    sns.heatmap(\n",
    "        probs_matrix,\n",
    "        annot=tokens_matrix,\n",
    "        fmt=\"\",\n",
    "        cmap=\"YlGnBu\",\n",
    "        xticklabels=expected_tokens,\n",
    "        yticklabels=[f\"L{i}\" for i in range(num_layers, 0, -1)],\n",
    "        cbar_kws={'label': 'Entropy'}\n",
    "    )\n",
    "    plt.title(f\"Logit Lens Heatmap\\nModel: {model_id}\\nQ: {question_text}\")\n",
    "    plt.xlabel(\"Generated Token\")\n",
    "    plt.ylabel(\"Transformer Layer\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return tokens_matrix, expected_tokens, question_text, model, tokenizer, inputs\n",
    "\n",
    "index = 0 \n",
    "\n",
    "tokens_matrix, expected_tokens, question_text, model, tokenizer, inputs = logit_lens_heatmap_from_file(\n",
    "    model_id=\"meta-llama/Llama-3.1-8B\",\n",
    "    index=index\n",
    ")\n",
    "\n",
    "consecutive_matches = count_consecutive_matches_before_layer_33(tokens_matrix)\n",
    "avg_stability = calculate_average_stability(consecutive_matches)\n",
    "accuracy = calculate_next_token_accuracy(model, tokenizer, inputs)\n",
    "\n",
    "print(\"\\n Antal sammenhængende matches op til lag 33:\")\n",
    "for token, count in consecutive_matches.items():\n",
    "    print(f\"{token.ljust(40)} | Matches: {count}\")\n",
    "\n",
    "print(f\"\\n Gennemsnitlig stabilitet: {avg_stability:.4f}\")\n",
    "print(f\" Next-token accuracy      : {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"hf_xxxxxxxxxx\")\n",
    "\n",
    "natural_questions_dataset = load_dataset(\"sentence-transformers/natural-questions\", split=\"train\")\n",
    "\n",
    "def fix_tokens(token_matrix):\n",
    "    for i in range(token_matrix.shape[0]):\n",
    "        for j in range(token_matrix.shape[1]):\n",
    "            token_matrix[i, j] = token_matrix[i, j].replace(\"$\", \"d\").strip()\n",
    "\n",
    "def get_custom_prompt(index=0):\n",
    "    item = natural_questions_dataset[index]\n",
    "    question = item[\"query\"]\n",
    "    answer = item[\"answer\"]\n",
    "    prompt = f\"Question: {question}\\nAnswer: {answer}\"\n",
    "    return prompt, answer, question\n",
    "\n",
    "def calculate_average_stability(consecutive_matches):\n",
    "    total = sum(consecutive_matches.values())\n",
    "    num_tokens = len(consecutive_matches)\n",
    "    return total / num_tokens if num_tokens > 0 else 0\n",
    "\n",
    "def count_consecutive_matches_before_layer_33(tokens_matrix):\n",
    "    num_layers, num_tokens = tokens_matrix.shape\n",
    "    final_tokens = [token.strip().lower() for token in tokens_matrix[0, :]]\n",
    "    consecutive_matches = {}\n",
    "\n",
    "    for token_idx in range(num_tokens):\n",
    "        target_token = final_tokens[token_idx]\n",
    "        count = 0\n",
    "        for layer_idx in range(0, num_layers):\n",
    "            predicted_token = tokens_matrix[layer_idx, token_idx].strip().lower()\n",
    "            if predicted_token == target_token:\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "        consecutive_matches[f\"Position {token_idx} ('{target_token}')\"] = count\n",
    "    return consecutive_matches\n",
    "\n",
    "def calculate_next_token_accuracy(model, tokenizer, inputs):\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits[:, :-1], dim=-1)\n",
    "    target_ids = input_ids[:, 1:]\n",
    "\n",
    "    correct = (predicted_ids == target_ids).sum().item()\n",
    "    total = target_ids.numel()\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "def logit_lens_heatmap_from_file(model_id=\"FreedomIntelligence/HuatuoGPT-o1-8B\", index=0):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=torch.float16, device_map=\"auto\"\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    prompt, expected_answer, question_text = get_custom_prompt(index=index)\n",
    "    print(f\"\\n Spørgsmål: {question_text}\")\n",
    "    print(f\" Forventet svar: {expected_answer}\")\n",
    "\n",
    "    full_input = prompt\n",
    "    inputs = tokenizer(full_input, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, output_scores=True)\n",
    "\n",
    "    all_hidden = outputs.hidden_states\n",
    "    num_layers = model.config.num_hidden_layers + 1\n",
    "    lm_head_weight = model.get_output_embeddings().weight.to(model.device)\n",
    "\n",
    "    tokens_matrix = []\n",
    "    probs_matrix = []\n",
    "\n",
    "    for layer in all_hidden:\n",
    "        logits = torch.matmul(layer, lm_head_weight.T)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        top_ids = torch.argmax(probs, dim=-1)\n",
    "        top_tokens = [tokenizer.decode(top_ids[0, i]) for i in range(top_ids.size(1))]\n",
    "        H = entropy(probs.detach().cpu().numpy(), axis=-1)\n",
    "        tokens_matrix.append(top_tokens)\n",
    "        probs_matrix.append(list(H[0]))\n",
    "\n",
    "    tokens_matrix = np.array(tokens_matrix)[::-1, :]\n",
    "    probs_matrix = np.array(probs_matrix)[::-1, :]\n",
    "\n",
    "    fix_tokens(tokens_matrix)\n",
    "\n",
    "    expected_tokens = [tokenizer.decode([t]) for t in inputs['input_ids'][0]]\n",
    "    expected_tokens = [token.strip() for token in expected_tokens]\n",
    "\n",
    "    plt.figure(figsize=(1 + len(expected_tokens), 10))\n",
    "    sns.heatmap(\n",
    "        probs_matrix,\n",
    "        annot=tokens_matrix,\n",
    "        fmt=\"\",\n",
    "        cmap=\"YlGnBu\",\n",
    "        xticklabels=expected_tokens,\n",
    "        yticklabels=[f\"L{i}\" for i in range(num_layers, 0, -1)],\n",
    "        cbar_kws={'label': 'Entropy'}\n",
    "    )\n",
    "    plt.title(f\"Logit Lens Heatmap\\nModel: {model_id}\\nQ: {question_text}\")\n",
    "    plt.xlabel(\"Generated Token\")\n",
    "    plt.ylabel(\"Transformer Layer\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return tokens_matrix, expected_tokens, question_text, model, tokenizer, inputs\n",
    "\n",
    "index = 0  \n",
    "\n",
    "tokens_matrix, expected_tokens, question_text, model, tokenizer, inputs = logit_lens_heatmap_from_file(\n",
    "    model_id=\"FreedomIntelligence/HuatuoGPT-o1-8B\",\n",
    "    index=index\n",
    ")\n",
    "\n",
    "consecutive_matches = count_consecutive_matches_before_layer_33(tokens_matrix)\n",
    "avg_stability = calculate_average_stability(consecutive_matches)\n",
    "accuracy = calculate_next_token_accuracy(model, tokenizer, inputs)\n",
    "\n",
    "print(\"\\n Antal sammenhængende matches op til lag 33:\")\n",
    "for token, count in consecutive_matches.items():\n",
    "    print(f\"{token.ljust(40)} | Matches: {count}\")\n",
    "\n",
    "print(f\"\\n Gennemsnitlig stabilitet: {avg_stability:.4f}\")\n",
    "print(f\" Next-token accuracy      : {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"hf_xxxxxxxxxx\")\n",
    "\n",
    "natural_questions_dataset = load_dataset(\"sentence-transformers/natural-questions\", split=\"train\")\n",
    "\n",
    "def fix_tokens(token_matrix):\n",
    "    for i in range(token_matrix.shape[0]):\n",
    "        for j in range(token_matrix.shape[1]):\n",
    "            token_matrix[i, j] = token_matrix[i, j].replace(\"$\", \"d\").strip()\n",
    "\n",
    "def get_custom_prompt(index=0):\n",
    "    item = natural_questions_dataset[index]\n",
    "    question = item[\"query\"]\n",
    "    answer = item[\"answer\"]\n",
    "    prompt = f\"Question: {question}\\nAnswer: {answer}\"\n",
    "    return prompt, answer, question\n",
    "\n",
    "def calculate_average_stability(consecutive_matches):\n",
    "    total = sum(consecutive_matches.values())\n",
    "    num_tokens = len(consecutive_matches)\n",
    "    return total / num_tokens if num_tokens > 0 else 0\n",
    "\n",
    "def count_consecutive_matches_before_layer_33(tokens_matrix):\n",
    "    num_layers, num_tokens = tokens_matrix.shape\n",
    "    final_tokens = [token.strip().lower() for token in tokens_matrix[0, :]]\n",
    "    consecutive_matches = {}\n",
    "\n",
    "    for token_idx in range(num_tokens):\n",
    "        target_token = final_tokens[token_idx]\n",
    "        count = 0\n",
    "        for layer_idx in range(0, num_layers):\n",
    "            predicted_token = tokens_matrix[layer_idx, token_idx].strip().lower()\n",
    "            if predicted_token == target_token:\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "        consecutive_matches[f\"Position {token_idx} ('{target_token}')\"] = count\n",
    "    return consecutive_matches\n",
    "\n",
    "def calculate_next_token_accuracy(model, tokenizer, inputs):\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits[:, :-1], dim=-1)\n",
    "    target_ids = input_ids[:, 1:]\n",
    "\n",
    "    correct = (predicted_ids == target_ids).sum().item()\n",
    "    total = target_ids.numel()\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "model_ids = {\n",
    "    \"LLaMA-3.1-8B\": \"meta-llama/Llama-3.1-8B\",\n",
    "    \"HuaTuo-o1-8B\": \"FreedomIntelligence/HuatuoGPT-o1-8B\" \n",
    "}\n",
    "num_questions = 5000\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, model_id in model_ids.items():\n",
    "    print(f\"\\n Starter analyse med model: {model_name}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=torch.float16, device_map=\"auto\"\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    avg_stabilities = []\n",
    "    accuracies = []\n",
    "\n",
    "    for index in range(num_questions):\n",
    "        try:\n",
    "            prompt, expected_answer, question_text = get_custom_prompt(index=index)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, output_hidden_states=True, output_scores=True)\n",
    "\n",
    "            all_hidden = outputs.hidden_states\n",
    "            num_layers = model.config.num_hidden_layers + 1\n",
    "            lm_head_weight = model.get_output_embeddings().weight.to(model.device)\n",
    "\n",
    "            tokens_matrix = []\n",
    "            for layer in all_hidden:\n",
    "                logits = torch.matmul(layer, lm_head_weight.T)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                top_ids = torch.argmax(probs, dim=-1)\n",
    "                top_tokens = [tokenizer.decode(top_ids[0, i]) for i in range(top_ids.size(1))]\n",
    "                tokens_matrix.append(top_tokens)\n",
    "\n",
    "            tokens_matrix = np.array(tokens_matrix)[::-1, :]\n",
    "            fix_tokens(tokens_matrix)\n",
    "\n",
    "            consecutive_matches = count_consecutive_matches_before_layer_33(tokens_matrix)\n",
    "            avg_stability = calculate_average_stability(consecutive_matches)\n",
    "            accuracy = calculate_next_token_accuracy(model, tokenizer, inputs)\n",
    "\n",
    "            avg_stabilities.append(avg_stability)\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "            if (index + 1) % 100 == 0:\n",
    "                print(f\"{index + 1} spørgsmål analyseret med {model_name}...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Fejl ved index {index} i model {model_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    results[model_name] = {\n",
    "        \"avg_stability\": np.mean(avg_stabilities),\n",
    "        \"accuracy\": np.mean(accuracies)\n",
    "    }\n",
    "\n",
    "print(\"\\n Sammenfatning af resultater:\")\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\n Model: {model_name}\")\n",
    "    print(f\" Gennemsnitlig stabilitet: {metrics['avg_stability']:.4f}\")\n",
    "    print(f\" Gennemsnitlig accuracy  : {metrics['accuracy']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
